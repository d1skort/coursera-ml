{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('wine.data.txt', header=None)\n",
    "data = data.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = data[:, 1:], data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-52-a34cf6157070>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-a34cf6157070>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    for k in range(1, 51):\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, Y_test = y[train_index], y[test_index]\n",
    "    for k in range(1, 51):\n",
    "        clf = KNeighborsClassifier(n_neighbors=k)\n",
    "        print(cross_val_score(estimator=clf, X=X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KNeighborsClassifier in module sklearn.neighbors.classification:\n",
      "\n",
      "class KNeighborsClassifier(sklearn.neighbors.base.NeighborsBase, sklearn.neighbors.base.KNeighborsMixin, sklearn.neighbors.base.SupervisedIntegerMixin, sklearn.base.ClassifierMixin)\n",
      " |  Classifier implementing the k-nearest neighbors vote.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_neighbors : int, optional (default = 5)\n",
      " |      Number of neighbors to use by default for :meth:`k_neighbors` queries.\n",
      " |  \n",
      " |  weights : str or callable, optional (default = 'uniform')\n",
      " |      weight function used in prediction.  Possible values:\n",
      " |  \n",
      " |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      " |        are weighted equally.\n",
      " |      - 'distance' : weight points by the inverse of their distance.\n",
      " |        in this case, closer neighbors of a query point will have a\n",
      " |        greater influence than neighbors which are further away.\n",
      " |      - [callable] : a user-defined function which accepts an\n",
      " |        array of distances, and returns an array of the same shape\n",
      " |        containing the weights.\n",
      " |  \n",
      " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
      " |      Algorithm used to compute the nearest neighbors:\n",
      " |  \n",
      " |      - 'ball_tree' will use :class:`BallTree`\n",
      " |      - 'kd_tree' will use :class:`KDTree`\n",
      " |      - 'brute' will use a brute-force search.\n",
      " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      " |        based on the values passed to :meth:`fit` method.\n",
      " |  \n",
      " |      Note: fitting on sparse input will override the setting of\n",
      " |      this parameter, using brute force.\n",
      " |  \n",
      " |  leaf_size : int, optional (default = 30)\n",
      " |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      " |      speed of the construction and query, as well as the memory\n",
      " |      required to store the tree.  The optimal value depends on the\n",
      " |      nature of the problem.\n",
      " |  \n",
      " |  metric : string or DistanceMetric object (default = 'minkowski')\n",
      " |      the distance metric to use for the tree.  The default metric is\n",
      " |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      " |      metric. See the documentation of the DistanceMetric class for a\n",
      " |      list of available metrics.\n",
      " |  \n",
      " |  p : integer, optional (default = 2)\n",
      " |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      " |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      " |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      " |  \n",
      " |  metric_params : dict, optional (default = None)\n",
      " |      Additional keyword arguments for the metric function.\n",
      " |  \n",
      " |  n_jobs : int, optional (default = 1)\n",
      " |      The number of parallel jobs to run for neighbors search.\n",
      " |      If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
      " |      Doesn't affect :meth:`fit` method.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> X = [[0], [1], [2], [3]]\n",
      " |  >>> y = [0, 0, 1, 1]\n",
      " |  >>> from sklearn.neighbors import KNeighborsClassifier\n",
      " |  >>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
      " |  >>> neigh.fit(X, y) # doctest: +ELLIPSIS\n",
      " |  KNeighborsClassifier(...)\n",
      " |  >>> print(neigh.predict([[1.1]]))\n",
      " |  [0]\n",
      " |  >>> print(neigh.predict_proba([[0.9]]))\n",
      " |  [[ 0.66666667  0.33333333]]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  RadiusNeighborsClassifier\n",
      " |  KNeighborsRegressor\n",
      " |  RadiusNeighborsRegressor\n",
      " |  NearestNeighbors\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      " |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      " |  \n",
      " |  .. warning::\n",
      " |  \n",
      " |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      " |     neighbors, neighbor `k+1` and `k`, have identical distances\n",
      " |     but different labels, the results will depend on the ordering of the\n",
      " |     training data.\n",
      " |  \n",
      " |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KNeighborsClassifier\n",
      " |      sklearn.neighbors.base.NeighborsBase\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.neighbors.base.KNeighborsMixin\n",
      " |      sklearn.neighbors.base.SupervisedIntegerMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the class labels for the provided data\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape [n_samples] or [n_samples, n_outputs]\n",
      " |          Class labels for each data sample.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test data X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          of such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. Classes are ordered\n",
      " |          by lexicographic order.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors.base.KNeighborsMixin:\n",
      " |  \n",
      " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      " |      Finds the K-neighbors of a point.\n",
      " |      \n",
      " |      Returns indices of and distances to the neighbors of each point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int\n",
      " |          Number of neighbors to get (default is the value\n",
      " |          passed to the constructor).\n",
      " |      \n",
      " |      return_distance : boolean, optional. Defaults to True.\n",
      " |          If False, distances will not be returned\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dist : array\n",
      " |          Array representing the lengths to points, only present if\n",
      " |          return_distance=True\n",
      " |      \n",
      " |      ind : array\n",
      " |          Indices of the nearest points in the population matrix.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      In the following example, we construct a NeighborsClassifier\n",
      " |      class from an array representing our data set and ask who's\n",
      " |      the closest point to [1,1,1]\n",
      " |      \n",
      " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      " |      >>> neigh.fit(samples) # doctest: +ELLIPSIS\n",
      " |      NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n",
      " |      >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS\n",
      " |      (array([[ 0.5]]), array([[2]]...))\n",
      " |      \n",
      " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      " |      element is at distance 0.5 and is the third element of samples\n",
      " |      (indexes start at 0). You can also query for multiple points:\n",
      " |      \n",
      " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      " |      >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS\n",
      " |      array([[1],\n",
      " |             [2]]...)\n",
      " |  \n",
      " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      " |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int\n",
      " |          Number of neighbors for each sample.\n",
      " |          (default is value passed to the constructor).\n",
      " |      \n",
      " |      mode : {'connectivity', 'distance'}, optional\n",
      " |          Type of returned matrix: 'connectivity' will return the\n",
      " |          connectivity matrix with ones and zeros, in 'distance' the\n",
      " |          edges are Euclidean distance between points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]\n",
      " |          n_samples_fit is the number of samples in the fitted data\n",
      " |          A[i, j] is assigned the weight of edge that connects i to j.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> X = [[0], [3], [1]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      " |      >>> neigh.fit(X) # doctest: +ELLIPSIS\n",
      " |      NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n",
      " |      >>> A = neigh.kneighbors_graph(X)\n",
      " |      >>> A.toarray()\n",
      " |      array([[ 1.,  0.,  1.],\n",
      " |             [ 0.,  1.,  1.],\n",
      " |             [ 1.,  0.,  1.]])\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      NearestNeighbors.radius_neighbors_graph\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors.base.SupervisedIntegerMixin:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model using X as training data and y as target values\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, BallTree, KDTree}\n",
      " |          Training data. If array or matrix, shape [n_samples, n_features],\n",
      " |          or [n_samples, n_samples] if metric='precomputed'.\n",
      " |      \n",
      " |      y : {array-like, sparse matrix}\n",
      " |          Target values of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(KNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KFold in module sklearn.model_selection._split:\n",
      "\n",
      "class KFold(_BaseKFold)\n",
      " |  K-Folds cross-validator\n",
      " |  \n",
      " |  Provides train/test indices to split data in train/test sets. Split\n",
      " |  dataset into k consecutive folds (without shuffling by default).\n",
      " |  \n",
      " |  Each fold is then used once as a validation while the k - 1 remaining\n",
      " |  folds form the training set.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=3\n",
      " |      Number of folds. Must be at least 2.\n",
      " |  \n",
      " |  shuffle : boolean, optional\n",
      " |      Whether to shuffle the data before splitting into batches.\n",
      " |  \n",
      " |  random_state : None, int or RandomState\n",
      " |      When shuffle=True, pseudo-random number generator state used for\n",
      " |      shuffling. If None, use default numpy RNG for shuffling.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.model_selection import KFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([1, 2, 3, 4])\n",
      " |  >>> kf = KFold(n_splits=2)\n",
      " |  >>> kf.get_n_splits(X)\n",
      " |  2\n",
      " |  >>> print(kf)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |  KFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for train_index, test_index in kf.split(X):\n",
      " |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      " |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      " |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      " |  TRAIN: [2 3] TEST: [0 1]\n",
      " |  TRAIN: [0 1] TEST: [2 3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The first ``n_samples % n_splits`` folds have size\n",
      " |  ``n_samples // n_splits + 1``, other folds have size\n",
      " |  ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  StratifiedKFold\n",
      " |      Takes group information into account to avoid building folds with\n",
      " |      imbalanced class distributions (for binary or multiclass\n",
      " |      classification tasks).\n",
      " |  \n",
      " |  GroupKFold: K-fold iterator variant with non-overlapping groups.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KFold\n",
      " |      _BaseKFold\n",
      " |      abc.NewBase\n",
      " |      BaseCrossValidator\n",
      " |      abc.NewBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_splits=3, shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |  \n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |  \n",
      " |  split(self, X, y=None, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |      \n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |      \n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from abc.NewBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(KFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cross_val_score in module sklearn.model_selection._validation:\n",
      "\n",
      "cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')\n",
      "    Evaluate a score by cross-validation\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    estimator : estimator object implementing 'fit'\n",
      "        The object to use to fit the data.\n",
      "    \n",
      "    X : array-like\n",
      "        The data to fit. Can be, for example a list, or an array at least 2d.\n",
      "    \n",
      "    y : array-like, optional, default: None\n",
      "        The target variable to try to predict in the case of\n",
      "        supervised learning.\n",
      "    \n",
      "    groups : array-like, with shape (n_samples,), optional\n",
      "        Group labels for the samples used while splitting the dataset into\n",
      "        train/test set.\n",
      "    \n",
      "    scoring : string, callable or None, optional, default: None\n",
      "        A string (see model evaluation documentation) or\n",
      "        a scorer callable object / function with signature\n",
      "        ``scorer(estimator, X, y)``.\n",
      "    \n",
      "    cv : int, cross-validation generator or an iterable, optional\n",
      "        Determines the cross-validation splitting strategy.\n",
      "        Possible inputs for cv are:\n",
      "          - None, to use the default 3-fold cross validation,\n",
      "          - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      "          - An object to be used as a cross-validation generator.\n",
      "          - An iterable yielding train, test splits.\n",
      "    \n",
      "        For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "        other cases, :class:`KFold` is used.\n",
      "    \n",
      "        Refer :ref:`User Guide <cross_validation>` for the various\n",
      "        cross-validation strategies that can be used here.\n",
      "    \n",
      "    n_jobs : integer, optional\n",
      "        The number of CPUs to use to do the computation. -1 means\n",
      "        'all CPUs'.\n",
      "    \n",
      "    verbose : integer, optional\n",
      "        The verbosity level.\n",
      "    \n",
      "    fit_params : dict, optional\n",
      "        Parameters to pass to the fit method of the estimator.\n",
      "    \n",
      "    pre_dispatch : int, or string, optional\n",
      "        Controls the number of jobs that get dispatched during parallel\n",
      "        execution. Reducing this number can be useful to avoid an\n",
      "        explosion of memory consumption when more jobs get dispatched\n",
      "        than CPUs can process. This parameter can be:\n",
      "    \n",
      "            - None, in which case all the jobs are immediately\n",
      "              created and spawned. Use this for lightweight and\n",
      "              fast-running jobs, to avoid delays due to on-demand\n",
      "              spawning of the jobs\n",
      "    \n",
      "            - An int, giving the exact number of total jobs that are\n",
      "              spawned\n",
      "    \n",
      "            - A string, giving an expression as a function of n_jobs,\n",
      "              as in '2*n_jobs'\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    scores : array of float, shape=(len(list(cv)),)\n",
      "        Array of scores of the estimator for each run of the cross validation.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn import datasets, linear_model\n",
      "    >>> from sklearn.model_selection import cross_val_score\n",
      "    >>> diabetes = datasets.load_diabetes()\n",
      "    >>> X = diabetes.data[:150]\n",
      "    >>> y = diabetes.target[:150]\n",
      "    >>> lasso = linear_model.Lasso()\n",
      "    >>> print(cross_val_score(lasso, X, y))  # doctest: +ELLIPSIS\n",
      "    [ 0.33150734  0.08022311  0.03531764]\n",
      "    \n",
      "    See Also\n",
      "    ---------\n",
      "    :func:`sklearn.metrics.make_scorer`:\n",
      "        Make a scorer from a performance metric or loss function.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
